# BayesEvolve: Technical Documentation & Improvement Roadmap

## Table of Contents
1. [Overview](#overview)
2. [Era 1: Analytical Era (Conjugate Priors)](#era-1-analytical-era)
3. [Era 2: Simulation Revolution (Metropolis-Hastings)](#era-2-simulation-revolution)
4. [Era 3: Component-Wise Era (Gibbs Sampling)](#era-3-component-wise-era)
5. [Era 4: Geometric Era (Hamiltonian Monte Carlo)](#era-4-geometric-era)
6. [Era 5: Scalable Era (Variational Inference)](#era-5-scalable-era)
7. [Cross-Cutting Improvements](#cross-cutting-improvements)

---

## Overview

BayesEvolve is an interactive educational platform teaching the historical evolution of Bayesian computation methods. Each "era" represents a breakthrough in computational statistics, visualized through interactive canvas-based simulations.

### Technical Stack
- **Framework**: React 18 with TypeScript
- **Rendering**: HTML5 Canvas API (2D context)
- **Styling**: Tailwind CSS
- **Math Rendering**: KaTeX
- **State Management**: React hooks + Context API
- **Internationalization**: Custom LanguageContext (EN/VI)

### Architecture Pattern
```
App.tsx
â”œâ”€â”€ LanguageProvider (i18n context)
â”œâ”€â”€ ProgressProvider (achievements/progress)
â””â”€â”€ Layout.tsx
    â”œâ”€â”€ ModuleHome.tsx (timeline overview)
    â”œâ”€â”€ ModuleMetropolis.tsx (Era 2)
    â”œâ”€â”€ ModuleGibbs.tsx (Era 3)
    â”œâ”€â”€ ModuleHMC.tsx (Era 4)
    â””â”€â”€ ModuleVariational.tsx (Era 5)
```

---

## Era 1: Analytical Era

### Current Implementation
**Status**: Timeline overview only (no dedicated interactive module)

The Analytical Era is presented in `ModuleHome.tsx` as an expandable card explaining conjugate priors. No interactive simulation exists.

### Mathematical Foundation
```
Posterior âˆ Likelihood Ã— Prior

Beta-Binomial:    Beta(Î±,Î²) + Binom(n,k) â†’ Beta(Î±+k, Î²+n-k)
Normal-Normal:    N(Î¼â‚€,Ïƒâ‚€Â²) + N(xÌ„,ÏƒÂ²/n) â†’ N(Î¼â‚™,Ïƒâ‚™Â²)
Gamma-Poisson:    Î“(Î±,Î²) + Pois(Î£k) â†’ Î“(Î±+Î£k, Î²+T)
```

### Suggested Interactive Module: `ModuleAnalytical.tsx`

#### Visualization Concept
Create a **Beta-Binomial Coin Flip Simulator**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Prior: Beta(Î±, Î²)           Posterior: Beta(Î±', Î²')â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   [Prior PDF] â”‚  â”€â”€â”€â”€â”€â”€â–º  â”‚ [Posterior]   â”‚      â”‚
â”‚  â”‚   Î±=2, Î²=2    â”‚   Data    â”‚  Î±'=Î±+k       â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚  Î²'=Î²+(n-k)   â”‚      â”‚
â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  ğŸª™ ğŸª™ ğŸª™ ğŸª™ ğŸª™ ğŸª™ ğŸª™ ğŸª™ ğŸª™ ğŸª™              â”‚   â”‚
â”‚  â”‚  Click to flip coins! Heads: 7, Tails: 3    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                     â”‚
â”‚  [Flip 1] [Flip 10] [Flip 100] [Reset]             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Implementation Details
```typescript
interface AnalyticalState {
  priorAlpha: number;      // Prior Beta Î± parameter
  priorBeta: number;       // Prior Beta Î² parameter
  heads: number;           // Observed heads
  tails: number;           // Observed tails
  trueProbability: number; // Hidden true coin bias (0-1)
}

// Posterior is Beta(Î± + heads, Î² + tails)
// Mean = (Î± + heads) / (Î± + Î² + heads + tails)
```

#### Educational Value
- **Shows**: How prior beliefs update with data
- **Key Insight**: With enough data, prior becomes irrelevant (posterior converges to MLE)
- **Limitation Demo**: Try non-conjugate likelihood (e.g., mixture) â†’ no closed form

#### Suggested Features
1. **Adjustable prior strength**: Slider for Î±, Î² from 0.1 to 100
2. **True probability reveal**: After n flips, show hidden true Î¸
3. **Credible interval visualization**: Show 95% CI shrinking with more data
4. **Comparison mode**: Show frequentist vs Bayesian estimates side-by-side
5. **"Break the conjugacy" button**: Switch to non-conjugate model, show why MCMC needed

---

## Era 2: Simulation Revolution (Metropolis-Hastings)

### Current Implementation
**File**: `components/ModuleMetropolis.tsx`

#### Technical Specifications
| Parameter | Value |
|-----------|-------|
| Canvas Size | 500 Ã— 500 pixels |
| Scale | 50 pixels/unit |
| Target Distribution | Bimodal 2D (custom `targetMapPdf`) |
| Proposal Distribution | Symmetric Gaussian (Ïƒ = 0.8) |
| Visualization | "Fog of war" clearing as explored |

#### Algorithm Implementation
```typescript
// Proposal step
const proposalX = position.x + (Math.random() - 0.5) * 1.6;
const proposalY = position.y + (Math.random() - 0.5) * 1.6;

// Acceptance ratio (symmetric proposal, so q cancels)
const currentProb = targetMapPdf(position.x, position.y);
const proposalProb = targetMapPdf(proposal.x, proposal.y);
const acceptanceRatio = proposalProb / currentProb;

// Accept/Reject
if (Math.random() < acceptanceRatio) {
  // Accept: move to proposal
} else {
  // Reject: stay in place
}
```

#### Current Features
- Manual mode: Step-by-step propose â†’ accept/reject
- Auto mode: Continuous sampling with speed control
- Fog of war: Reveals explored regions
- Accept rate counter
- Sample history trail

### Improvement Suggestions

#### 1. **Proposal Distribution Visualization**
**Problem**: Students don't see *where* proposals come from.

**Solution**: Draw proposal distribution as a translucent circle around current position.
```typescript
// Draw proposal region
ctx.beginPath();
ctx.arc(currentX, currentY, proposalSigma * SCALE, 0, Math.PI * 2);
ctx.fillStyle = 'rgba(59, 130, 246, 0.1)'; // Light blue
ctx.fill();
ctx.strokeStyle = 'rgba(59, 130, 246, 0.5)';
ctx.setLineDash([5, 5]);
ctx.stroke();
```

#### 2. **Acceptance Probability Meter**
**Problem**: Students don't understand *why* some proposals are rejected.

**Solution**: Add real-time display showing:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Current Ï€(Î¸):     0.0342           â”‚
â”‚ Proposal Ï€(Î¸'):   0.0891           â”‚
â”‚ Ratio Ï€(Î¸')/Ï€(Î¸): 2.60 (>1 â†’ âœ“)   â”‚
â”‚ Random u:         0.45             â”‚
â”‚ Decision:         ACCEPT âœ“         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3. **Adjustable Proposal Width**
**Problem**: Students don't understand tuning importance.

**Solution**: Add slider for proposal Ïƒ with live feedback:
- **Ïƒ too small**: High acceptance, slow exploration (show ESS)
- **Ïƒ too large**: Low acceptance, stuck in place
- **Ïƒ optimal**: ~23-44% acceptance rate (show Goldilocks zone)

```typescript
const [proposalSigma, setProposalSigma] = useState(0.8);

// Show acceptance rate color coding
const getAcceptanceColor = (rate: number) => {
  if (rate < 0.15) return 'text-red-500';      // Too large Ïƒ
  if (rate > 0.50) return 'text-yellow-500';   // Too small Ïƒ
  return 'text-green-500';                      // Optimal
};
```

#### 4. **Multiple Target Distributions**
**Problem**: Single distribution doesn't show generality.

**Solution**: Dropdown to select different targets:
- **Unimodal Gaussian**: Easy case
- **Bimodal**: Current default, shows mode-hopping challenge
- **Banana-shaped**: Shows correlation issues
- **Donut**: Shows multimodal ring

#### 5. **Burn-in Visualization**
**Problem**: Students don't understand burn-in concept.

**Solution**: 
- Color early samples differently (gray â†’ brown gradient)
- Add "discard burn-in" button that removes first N samples
- Show histogram with/without burn-in

#### 6. **Effective Sample Size (ESS)**
**Problem**: Students think more samples = better, ignoring autocorrelation.

**Solution**: Display ESS alongside total samples:
```
Samples: 1000
ESS: 127 (autocorrelation = 0.87)
Efficiency: 12.7%
```

#### 7. **Real-World Connection: Bayesian Regression**
**Problem**: Abstract 2D landscape doesn't connect to real problems.

**Solution**: Add "Applied Mode" showing:
- Simple linear regression: y = Î²â‚€ + Î²â‚x + Îµ
- 2D canvas shows posterior over (Î²â‚€, Î²â‚)
- Data points shown alongside
- Students can add/remove data and watch posterior update

---

## Era 3: Component-Wise Era (Gibbs Sampling)

### Current Implementation
**File**: `components/ModuleGibbs.tsx`

#### Technical Specifications
| Parameter | Value |
|-----------|-------|
| Canvas Size | 500 Ã— 500 pixels |
| Scale | 50 pixels/unit |
| Target Distribution | Bivariate Normal |
| Correlation (Ï) | Adjustable 0 to 0.99 |

#### Algorithm Implementation
```typescript
// Conditional distributions for bivariate normal with Ï:
// X | Y=y ~ N(Ïy, 1-ÏÂ²)
// Y | X=x ~ N(Ïx, 1-ÏÂ²)

const sd = Math.sqrt(1 - rho * rho);

if (turn === 'X') {
  const mean = rho * position.y;
  newPos.x = mean + z * sd;  // z ~ N(0,1)
} else {
  const mean = rho * position.x;
  newPos.y = mean + z * sd;
}
```

#### Current Features
- Alternating X|Y and Y|X sampling
- Adjustable correlation Ï
- Visual indicator of which variable is fixed/sampled
- Manual and auto modes
- Sample history trail

### Improvement Suggestions

#### 1. **Conditional Distribution Slice Visualization**
**Problem**: Students don't see *what* conditional distribution looks like.

**Solution**: When sampling X|Y, draw the 1D conditional as a curve:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              2D Contour                â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚     â”‚     Current slice (Y=1.5)   â”‚    â”‚
â”‚     â”‚        â†“â†“â†“â†“â†“â†“â†“â†“            â”‚    â”‚
â”‚     â”‚  â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•       â”‚    â”‚
â”‚     â”‚        â˜… (sample)           â”‚    â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                        â”‚
â”‚  1D Conditional: p(X | Y=1.5)          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚      âˆ©                         â”‚   â”‚
â”‚  â”‚     / \    â† Sample here       â”‚   â”‚
â”‚  â”‚    /   \                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2. **Correlation Pathology Demo**
**Problem**: Students don't viscerally feel why high Ï is bad.

**Solution**: Add metrics panel showing:
```
Ï = 0.95
Conditional variance: ÏƒÂ²(1-ÏÂ²) = 0.0975
Steps to traverse 1 unit: ~10.3
Mixing time: ~106 steps

âš ï¸ High correlation detected!
   Chain is "stuck" in narrow diagonal.
```

#### 3. **Side-by-Side Comparison**
**Problem**: Hard to compare Gibbs vs M-H on same problem.

**Solution**: Split-screen mode:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Gibbs (left)  â”‚    M-H (right)  â”‚
â”‚                 â”‚                 â”‚
â”‚  Samples: 100   â”‚  Samples: 100   â”‚
â”‚  ESS: 45        â”‚  ESS: 23        â”‚
â”‚                 â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 4. **Higher Dimensions Preview**
**Problem**: 2D doesn't show the curse of dimensionality.

**Solution**: Add "Dimension Simulator":
- Show how acceptance rate / ESS degrades with d
- Plot: ESS vs dimensions (2, 5, 10, 20, 50)
- Message: "In 100D, Gibbs takes 10,000Ã— longer!"

#### 5. **Block Gibbs Extension**
**Problem**: Students don't learn about block updates.

**Solution**: Add toggle for "Block Gibbs":
- Sample (X,Y) jointly from 2D conditional
- Show improved mixing when variables correlated
- Compare: Full conditionals vs Block updates

#### 6. **Real-World Connection: Bayesian Linear Regression**
**Problem**: Bivariate normal is too abstract.

**Solution**: Show Gibbs for regression:
```
Model: y = XÎ² + Îµ, Îµ ~ N(0, ÏƒÂ²)
       Î² ~ N(0, Ï„Â²I)
       ÏƒÂ² ~ InvGamma(a, b)

Gibbs updates:
1. Î² | ÏƒÂ², y ~ N(posterior mean, posterior cov)
2. ÏƒÂ² | Î², y ~ InvGamma(a', b')
```
- Show regression line updating as sampler runs
- Data points visible on scatter plot

#### 7. **Rao-Blackwellization Demo**
**Problem**: Advanced concept not covered.

**Solution**: Show that using conditional means reduces variance:
- Plot: Sample histogram vs Rao-Blackwell estimate
- Message: "Using E[X|Y] instead of X samples gives 2Ã— efficiency!"

---

## Era 4: Geometric Era (Hamiltonian Monte Carlo)

### Current Implementation
**File**: `components/ModuleHMC.tsx`

#### Technical Specifications
| Parameter | Value |
|-----------|-------|
| Canvas Size | 600 Ã— 500 pixels |
| Scale | 40 pixels/unit |
| Target Distribution | "Donut" ring: U(q) = (â€–qâ€– - R)Â² |
| Integrator | Leapfrog (StÃ¶rmer-Verlet) |
| Time step (dt) | 0.05 |
| Mass | 1.0 |

#### Algorithm Implementation
```typescript
// Leapfrog integrator for Hamiltonian dynamics
// H(q,p) = U(q) + K(p) = -log Ï€(q) + pÂ²/2m

// Half step momentum
const grad1 = donutGradient(q.x, q.y);
let px = p.x - (dt / 2) * grad1.dx;
let py = p.y - (dt / 2) * grad1.dy;

// Full step position
const qx = q.x + dt * (px / mass);
const qy = q.y + dt * (py / mass);

// Half step momentum
const grad2 = donutGradient(qx, qy);
px = px - (dt / 2) * grad2.dx;
py = py - (dt / 2) * grad2.dy;
```

#### Current Features
- Drag-to-flick momentum initialization
- Continuous trajectory visualization
- Sample collection after fixed time
- Momentum magnitude display

### Improvement Suggestions

#### 1. **Energy Conservation Visualization**
**Problem**: Students don't see why leapfrog is symplectic.

**Solution**: Add energy panel:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Hamiltonian Conservation            â”‚
â”‚                                     â”‚
â”‚ H(q,p) = U(q) + K(p)               â”‚
â”‚                                     â”‚
â”‚ Initial H:  2.345                   â”‚
â”‚ Current H:  2.351                   â”‚
â”‚ Î”H:         0.006 (0.26%)          â”‚
â”‚                                     â”‚
â”‚ [Energy vs Time Plot]               â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                â”‚
â”‚ H should stay ~constant!            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2. **Step Size Instability Demo**
**Problem**: Students don't understand numerical stability.

**Solution**: Add dt slider with warnings:
- **dt = 0.01**: Stable but slow (show trajectory)
- **dt = 0.05**: Good balance (default)
- **dt = 0.2**: Unstable! (show energy exploding)
- **dt = 0.5**: Complete divergence (trajectory flies off)

```typescript
const getStabilityWarning = (dt: number) => {
  if (dt > 0.15) return 'âš ï¸ Approaching instability!';
  if (dt > 0.25) return 'ğŸ”¥ UNSTABLE - Energy diverging!';
  return 'âœ“ Stable';
};
```

#### 3. **Trajectory Length Tuning**
**Problem**: Fixed 3-second trajectory doesn't teach tuning.

**Solution**: Add L (number of leapfrog steps) slider:
- **L too small**: Doesn't explore far, like random walk
- **L too large**: U-turn, wasted computation
- **L optimal**: Reaches opposite side of distribution

Show "U-turn" detection:
```
Trajectory is turning back!
NUTS would stop here to save computation.
```

#### 4. **Phase Space Visualization**
**Problem**: Students only see position space.

**Solution**: Split view showing both:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Position (q)   â”‚  Momentum (p)   â”‚
â”‚                 â”‚                 â”‚
â”‚    [Donut]      â”‚   [Momentum]    â”‚
â”‚      â—â†’         â”‚      â—          â”‚
â”‚                 â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 5. **Gradient Visualization**
**Problem**: Students don't see how gradient guides motion.

**Solution**: Draw gradient arrows on the potential surface:
```typescript
// Draw gradient field
for (let x = -5; x <= 5; x += 1) {
  for (let y = -5; y <= 5; y += 1) {
    const grad = donutGradient(x, y);
    drawArrow(ctx, x, y, -grad.dx, -grad.dy); // Points downhill
  }
}
```

#### 6. **Comparison: HMC vs Random Walk**
**Problem**: Students don't appreciate HMC's efficiency.

**Solution**: Side-by-side race:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     HMC         â”‚  Random Walk    â”‚
â”‚                 â”‚                 â”‚
â”‚  ESS: 95        â”‚  ESS: 12        â”‚
â”‚  Time: 1.2s     â”‚  Time: 1.2s     â”‚
â”‚                 â”‚                 â”‚
â”‚  "HMC is 8Ã— more efficient!"      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 7. **Different Potential Surfaces**
**Problem**: Only donut doesn't show generality.

**Solution**: Multiple targets:
- **Gaussian**: Simple bowl
- **Donut**: Current default
- **Banana**: Highly correlated (show HMC handles it)
- **Funnel**: Neal's funnel (show mass matrix importance)

#### 8. **Mass Matrix Adaptation**
**Problem**: Constant mass doesn't show importance of preconditioning.

**Solution**: Add "Adapt Mass" button:
- Start with M = I (identity)
- After warmup, estimate covariance
- Set M = Î£â»Â¹
- Show improved sampling in correlated distributions

#### 9. **NUTS Preview**
**Problem**: Students don't learn about No-U-Turn Sampler.

**Solution**: Add "NUTS Mode" toggle:
- Automatically detect U-turns
- Show tree-building process
- Display: "NUTS chose L=23 steps (adaptive!)"

#### 10. **Real-World Connection: Funnel Distribution**
**Problem**: Abstract donut doesn't connect to real problems.

**Solution**: Show hierarchical model posterior:
```
Î¸ ~ N(0, ÏƒÂ²)
Ïƒ ~ HalfCauchy(1)

This creates a "funnel" shape that's
notoriously hard for random walk MCMC
but HMC handles well with mass adaptation.
```

---

## Era 5: Scalable Era (Variational Inference)

### Current Implementation
**File**: `components/ModuleVariational.tsx`

#### Technical Specifications
| Parameter | Value |
|-----------|-------|
| Canvas Size | 500 Ã— 200 pixels |
| True Posterior | Bimodal mixture of Gaussians |
| Variational Family | Single Gaussian q(Î¸) = N(Î¼, ÏƒÂ²) |
| Optimization | Gradient ascent on ELBO |
| Learning Rate | 0.1 (Î¼), 0.05 (Ïƒ) |

#### Algorithm Implementation
```typescript
// True posterior: mixture of two Gaussians
const truePosterior = (x: number) => {
  return 0.4 * gaussian(x, 2, 0.8) + 0.6 * gaussian(x, 5, 1.2);
};

// ELBO approximation (simplified)
const computeELBO = (mu: number, sigma: number) => {
  // Monte Carlo estimate of E_q[log p(x)] - KL(q || prior)
  // Simplified for visualization
};

// Gradient ascent
const gradMu = (elbo(mu + Îµ) - elbo(mu - Îµ)) / (2 * Îµ);
const gradSigma = (elbo(mu, sigma + Îµ) - elbo(mu, sigma - Îµ)) / (2 * Îµ);

mu += learningRate * gradMu;
sigma += learningRate * gradSigma;
```

#### Current Features
- Manual Î¼ and Ïƒ sliders
- ELBO and KL divergence display
- Gradient ascent optimization
- Iteration counter
- Visual comparison of q(Î¸) vs p(Î¸|D)

### Improvement Suggestions

#### 1. **ELBO Decomposition Visualization**
**Problem**: Students don't understand what ELBO measures.

**Solution**: Break down ELBO components:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ELBO = E_q[log p(D|Î¸)] - KL(q || p(Î¸)) â”‚
â”‚                                         â”‚
â”‚ Likelihood term:  +2.34                 â”‚
â”‚ KL penalty:       -0.89                 â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€               â”‚
â”‚ ELBO:             +1.45                 â”‚
â”‚                                         â”‚
â”‚ [Stacked bar chart showing terms]       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 2. **KL Divergence Asymmetry Demo**
**Problem**: Students don't understand KL(q||p) vs KL(p||q).

**Solution**: Toggle between forward/reverse KL:
- **KL(q||p)**: Mode-seeking (VI default) - q avoids low p regions
- **KL(p||q)**: Mean-seeking - q covers all of p

Show visually:
```
KL(q||p): q focuses on ONE mode
KL(p||q): q tries to cover BOTH modes (too wide)
```

#### 3. **Variational Family Comparison**
**Problem**: Only single Gaussian shown.

**Solution**: Multiple variational families:
- **Single Gaussian**: Current (shows limitation)
- **Mixture of Gaussians**: Better multimodal fit
- **Normalizing Flow**: Even better (animated transformation)

```typescript
type VariationalFamily = 
  | 'gaussian' 
  | 'mixture_2' 
  | 'mixture_5' 
  | 'normalizing_flow';
```

#### 4. **Mean-Field vs Full Covariance**
**Problem**: Students don't see independence assumption cost.

**Solution**: 2D example with correlated posterior:
```
True posterior: Bivariate Normal with Ï = 0.9

Mean-field VI: q(x,y) = q(x)q(y) 
  â†’ Misses correlation entirely!

Full-rank VI: q(x,y) = N(Î¼, Î£)
  â†’ Captures correlation
```

#### 5. **Stochastic VI (SVI) Demo**
**Problem**: Batch gradient doesn't show scalability advantage.

**Solution**: Add "Mini-batch Mode":
- Show full dataset (1000 points)
- SVI uses random subset (32 points per step)
- Compare: Batch (slow, stable) vs SVI (fast, noisy)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Batch VI         â”‚  Stochastic VI  â”‚
â”‚ 1000 pts/step    â”‚  32 pts/step    â”‚
â”‚ Time: 10s        â”‚  Time: 0.5s     â”‚
â”‚ ELBO: smooth     â”‚  ELBO: noisy    â”‚
â”‚                  â”‚  but converges! â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 6. **Reparameterization Trick Visualization**
**Problem**: Key trick not explained visually.

**Solution**: Show the transformation:
```
Instead of: Î¸ ~ q(Î¸)
We write:   Î¸ = Î¼ + ÏƒÂ·Îµ, where Îµ ~ N(0,1)

This makes gradient âˆ‡_Î¼,Ïƒ computable!

[Animation showing Îµ samples transforming to Î¸ samples]
```

#### 7. **Amortized Inference Preview**
**Problem**: Per-datapoint VI not shown.

**Solution**: Add "Encoder Network" mode:
- Show neural network mapping x â†’ (Î¼(x), Ïƒ(x))
- This is the foundation of VAEs!
- Visualization: Input image â†’ latent distribution

#### 8. **ELBO Landscape Visualization**
**Problem**: Optimization landscape not visible.

**Solution**: 2D heatmap of ELBO(Î¼, Ïƒ):
```
      Ïƒ
      â†‘
      â”‚  â–‘â–‘â–‘â–“â–“â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–“â–“â–‘â–‘â–‘
      â”‚  â–‘â–‘â–“â–“â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–“â–“â–‘â–‘
      â”‚  â–‘â–“â–“â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–“â–“â–‘
      â”‚  â–“â–“â–ˆâ–ˆâ–ˆâ–ˆ â˜… â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–“â–“  â† Current (Î¼,Ïƒ)
      â”‚  â–“â–“â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–“
      â”‚  â–‘â–“â–“â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Î¼
      
      Gradient ascent path: â—‹â†’â—‹â†’â—‹â†’â˜…
```

#### 9. **VI vs MCMC Comparison**
**Problem**: No direct comparison with MCMC.

**Solution**: Split-screen race:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       VI        â”‚      MCMC       â”‚
â”‚                 â”‚                 â”‚
â”‚  Time: 0.1s     â”‚  Time: 10s      â”‚
â”‚  KL: 0.15       â”‚  KL: 0.02       â”‚
â”‚                 â”‚                 â”‚
â”‚  Fast but       â”‚  Slow but       â”‚
â”‚  approximate    â”‚  exact          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Trade-off: Speed vs Accuracy
```

#### 10. **Real-World Connection: Topic Modeling (LDA)**
**Problem**: Abstract 1D example doesn't show real use.

**Solution**: Mini topic model demo:
- 5 documents, 3 topics
- Show variational updates for topic proportions
- Visualize: Document â†’ Topic distribution â†’ Words

---

## Cross-Cutting Improvements

### 1. **Unified Progress System**
```typescript
interface LearningProgress {
  completedExercises: string[];
  achievements: Achievement[];
  quizScores: Record<string, number>;
  timeSpent: Record<string, number>;
}
```

### 2. **Interactive Quizzes After Each Era**
```
Quiz: Metropolis-Hastings
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Q1: Why can we use unnormalized densities?
â—‹ A) We only need ratios
â—‹ B) The normalizer is always 1
â—‹ C) It doesn't matter
â—‹ D) We estimate Z with samples

[Submit] [Hint]
```

### 3. **Code Export Feature**
```python
# Generated Python code for your simulation
import numpy as np

def metropolis_hastings(target_pdf, n_samples=1000, proposal_std=0.8):
    samples = []
    current = np.array([0.0, 0.0])
    
    for _ in range(n_samples):
        proposal = current + np.random.normal(0, proposal_std, 2)
        
        acceptance_ratio = target_pdf(proposal) / target_pdf(current)
        
        if np.random.random() < acceptance_ratio:
            current = proposal
        
        samples.append(current.copy())
    
    return np.array(samples)
```

### 4. **Difficulty Levels**
- **Beginner**: Simplified explanations, guided mode only
- **Intermediate**: Full manual/auto modes, basic math
- **Advanced**: All features, full math, edge cases

### 5. **Accessibility Improvements**
- Color-blind friendly palettes
- Screen reader support
- Keyboard navigation
- Reduced motion mode

### 6. **Mobile Responsiveness**
- Touch-friendly controls
- Responsive canvas sizing
- Swipe between eras

### 7. **Shareable State URLs**
```
https://bayesevolve.app/metropolis?sigma=0.5&samples=1000
```

### 8. **Real Dataset Integration**
Connect to real datasets:
- Iris (classification posterior)
- Boston Housing (regression)
- MNIST digits (high-dimensional)

---

## Implementation Priority Matrix

| Feature | Impact | Effort | Priority |
|---------|--------|--------|----------|
| Era 1 Module | High | Medium | P1 |
| Proposal visualization (Era 2) | High | Low | P1 |
| Conditional slice (Era 3) | High | Low | P1 |
| Energy conservation (Era 4) | High | Low | P1 |
| ELBO decomposition (Era 5) | High | Low | P1 |
| Step size instability (Era 4) | Medium | Low | P2 |
| KL asymmetry (Era 5) | Medium | Medium | P2 |
| Side-by-side comparisons | High | Medium | P2 |
| Real-world examples | High | High | P3 |
| Code export | Medium | Medium | P3 |
| Quizzes | Medium | High | P3 |

---

## Conclusion

BayesEvolve provides a strong foundation for teaching Bayesian computation. The suggested improvements focus on:

1. **Visualizing hidden mechanics** (proposals, conditionals, gradients)
2. **Showing failure modes** (high correlation, numerical instability)
3. **Connecting to real problems** (regression, topic models)
4. **Enabling comparisons** (MCMC vs VI, Gibbs vs M-H)
5. **Supporting different learning levels** (beginner â†’ advanced)

Implementing the P1 priorities would significantly enhance student comprehension with relatively low development effort.

---

*Document Version: 1.0*
*Last Updated: December 2024*
*Author: BayesEvolve Development Team*
